{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ΜΕΛΗ ΟΜΑΔΑΣ: \\\n",
    "ΚΑΓΙΑΤΣΚΑ ΕΡΙΚ - 1115202100043 \\\n",
    "ΚΑΛΑΜΠΟΚΗΣ ΕΥΑΓΓΕΛΟΣ - 1115202100045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 93\n",
      "Most common words:\n",
      " ['apartment', 'great', 'us', 'athens', 'stay', 'place', 'location', 'host', 'everything', 'clean', 'nice', 'would', 'recommend', 'really', 'good', 'well', 'perfect', 'helpful', 'also', 'acropolis', 'close', 'walk', 'time', 'comfortable', 'metro', 'restaurants', 'flat', 'amazing', 'area', 'city', 'even', 'easy', 'view', 'walking', 'highly', 'definitely', 'wonderful', 'located', 'one', 'around', 'need', 'lovely', 'get', 'like', 'home', 'airport', 'beautiful', 'best', 'distance', 'back', 'made', 'night', 'spacious', 'could', 'neighborhood', 'quiet', 'br', 'needed', 'excellent', 'staying', 'house', 'friendly', 'experience', 'minutes', 'thank', 'station', 'room', 'gave', 'much', 'day', 'enjoyed', 'stayed', 'away', 'street', 'go', 'fantastic', 'thanks', 'right', 'many', 'places', 'loved', 'within', 'little', 'kind', 'kitchen', 'central', 'visit', 'center', 'two', 'super', 'feel', 'felt', 'recommended', 'balcony', 'airbnb', 'see', 'arrived', 'local', 'lot', 'bed', 'safe', 'taxi', 'provided', 'space', 'arrival', 'small', 'next', 'trip', 'make', 'first', 'bars', 'communication', 'convenient', 'near', 'short', 'didnt', 'come', 'days', 'met', 'better', 'main', 'equipped', 'shops', 'always', 'people', 'lots', 'family', 'sites', 'check', 'sure', 'way', 'extremely', 'plaka', 'every', 'find', 'want', 'brthe', 'enough', 'terrace', 'late', 'nearby', 'appartment', 'minute', 'food', 'help', 'dimitris', 'welcoming', 'welcome', 'big', 'neighbourhood', 'absolutely', 'meet', 'exactly', 'modern', 'bit', 'questions', 'information', 'quite', 'pictures', 'coffee', 'attractions', 'things', 'hosts', 'bus', 'recommendations', 'helped', 'tips', 'bakery', 'views', 'available', 'anyone', 'take', 'breakfast', 'etc', 'centre', 'use', 'bathroom', 'eat', 'though', 'left', 'got', 'went', 'overall', 'quick', 'min', 'took', 'amenities', 'ralou', 'cozy', 'tourist', 'cafes', 'square', 'air', 'building', 'sights', 'living', 'nights', 'greek', 'warm', 'wifi', 'morning', 'able', 'decorated', 'full', 'found', 'dont', 'responsive', 'shower', 'arranged', 'person', 'large', 'described', 'plenty', 'hot', 'ever', 'friends', 'outside', 'aris', 'accommodating', 'greece', 'bedroom', 'far', 'couldnt', 'public', 'showed', 'romanos', 'transportation', 'door', 'easily', 'perfectly', 'awesome', 'museum', 'shopping', 'enjoy', 'couple', 'cool', 'friend', 'restaurant', 'fotis', 'transport', 'looking', 'major', 'part', 'welcomed', 'return', 'water', 'town', 'walked', 'supermarket', 'pleasant', 'love', 'heart', 'problem', 'know', 'anything', 'market', 'photos', 'corner', 'value', 'new', 'haris', 'including', 'syntagma', 'across', 'floor', 'wine', 'cant', 'although', 'worked', 'book', 'everywhere', 'appreciated', 'cosy', 'long', 'hospitality', 'early', 'especially', 'spent', 'say', 'map', 'going', 'top', 'never', 'price', 'anna', 'access', 'visiting', 'stop', 'rooftop', 'fully', 'waiting', 'driver', 'flight', 'incredible', 'ilias', 'maria', 'old', 'thing', 'happy', 'still']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "train_2019_df = pd.read_csv('data_train/train_2019.csv')\n",
    "train_2023_df = pd.read_csv('data_train/train_2023.csv')\n",
    "\n",
    "# Concatenate them in one dataframe\n",
    "df = pd.concat([train_2019_df, train_2023_df], ignore_index=True)\n",
    "\n",
    "# We want to drop lanes that are duplicate\n",
    "initial_row_count = df.shape[0]                                 # Initial row count\n",
    "df = df.drop_duplicates(subset=['id', 'comments'], keep='first')  # Remove duplicates that are shown in both 2019 and 2023\n",
    "final_row_count = df.shape[0]                                   # Final row count\n",
    "rows_dropped = initial_row_count - final_row_count              # Rows dropped\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")                # Print number of rows dropped\n",
    "\n",
    "# Edit comments\n",
    "comments = df['comments'].dropna().astype(str).tolist()\n",
    "\n",
    "# Add all comments in one text\n",
    "all_comments = ' '.join(comments)\n",
    "\n",
    "# Remove special characters etc.\n",
    "words = re.findall(r'\\b\\w+\\b', all_comments.lower())\n",
    "\n",
    "# Find how many times each word exists by counting them\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 300 most used words\n",
    "most_common_words = [word for word, count in word_counts.most_common(300)]\n",
    "\n",
    "# Print them\n",
    "print(\"Most common words:\\n\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Train Word2Vec model with all words\n",
    "model = Word2Vec([words], vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# embeddings for most used words\n",
    "embeddings = {word: model.wv[word] for word in most_common_words if word in model.wv}\n",
    "\n",
    "# similarity matrix\n",
    "word_list = list(embeddings.keys())\n",
    "vectors = np.array([embeddings[word] for word in word_list])\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Create dataframe with similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=word_list, columns=word_list)\n",
    "\n",
    "# Save to pickle file\n",
    "similarity_df.to_pickle('data_similarity/similarity_matrix.pkl')\n",
    "\n",
    "# Save also to csv file\n",
    "similarity_df.to_csv('data_similarity/similarity_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'apartment' embedding:\n",
      " [-0.08883116 -0.11179578  0.15214497 -0.1346948   0.00311453 -0.06754884\n",
      "  0.20030154  0.33600235 -0.30232838 -0.20672289  0.0774607  -0.2883072\n",
      "  0.05963396  0.12183825 -0.03778592  0.11020812  0.09395687  0.04795019\n",
      " -0.32755166 -0.15530117  0.09946761  0.19695462  0.26405153 -0.18531065\n",
      " -0.00494699 -0.02430619 -0.27107126 -0.01693872 -0.36834285  0.04933509\n",
      " -0.17254475 -0.10483955  0.03162792 -0.11344472  0.09230474  0.21511403\n",
      "  0.15855888  0.1168959   0.17786603 -0.05843613  0.29568493 -0.00688602\n",
      " -0.10350201  0.01167428  0.34207278 -0.08155998 -0.10743887  0.06838772\n",
      "  0.160287    0.0640679 ]\n",
      "\n",
      "'example' embedding:\n",
      " [ 0.44853713 -0.23952605 -0.81495648 -0.76954004  0.85065496 -0.4022519\n",
      "  0.91092584 -0.30012776 -0.88270384 -0.37424423 -0.92179863 -0.815033\n",
      "  0.04999589  0.70865638  0.37688544 -0.6770899   0.848643   -0.69424419\n",
      "  0.12622742 -0.54933092  0.44431685 -0.49162514 -0.12467297  0.22791623\n",
      " -0.72114342 -0.36445524  0.41353335 -0.83533108  0.63773725  0.34006409\n",
      " -0.30824639  0.25883786  0.28508714  0.51789671  0.91046996 -0.77034231\n",
      " -0.90732005 -0.36639476 -0.60495327  0.05269488 -0.83065929 -0.21040611\n",
      " -0.65477663 -0.3397735   0.19770919 -0.6434539  -0.20534527 -0.22339692\n",
      "  0.89969691 -0.23296906]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_word_embedding(word, embedding_dict, vector_size=50):\n",
    "    if word in embedding_dict:\n",
    "        return embedding_dict[word]\n",
    "    else:\n",
    "        return np.random.uniform(-1, 1, vector_size)\n",
    "\n",
    "# Example of word that exists\n",
    "word = 'apartment'\n",
    "embedding = get_word_embedding(word, embeddings)\n",
    "print(\"'apartment' embedding:\\n\", embedding)\n",
    "\n",
    "# Example of word that does not exist\n",
    "word = 'example'\n",
    "embedding = get_word_embedding(word, embeddings)\n",
    "print(\"\\n'example' embedding:\\n\", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(word1, word2, df_similarity):\n",
    "    # Check if words are in the DataFrame index \n",
    "    if word1 in df_similarity.index and word2 in df_similarity.index:\n",
    "        return df_similarity.loc[word1, word2]\n",
    "    else:\n",
    "        emb1 = get_word_embedding(word1, embeddings)\n",
    "        emb2 = get_word_embedding(word2, embeddings)\n",
    "        cos = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "        return cos\n",
    "    \n",
    "word1 = \"apartment\"\n",
    "word2 = \"example\"\n",
    "print(f\"Similarity between {word1} and {word2} is:\")\n",
    "print(word_similarity(word1, word2, similarity_df))     # We expect each time different result because words do not exist\n",
    "\n",
    "word3 = \"apartment\"\n",
    "word4 = \"apartment\"\n",
    "print(f\"Similarity between {word3} and {word4} is:\")    # We expect same result because words exist.\n",
    "print(word_similarity(word3, word4, similarity_df))     # Also approximately 1 because we have the same 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Neighbors\n",
    "def top_n_neighbors(word, sm_df, N):\n",
    "    if word in sm_df.index:\n",
    "        # Get the row corresponding to the word from sm_df\n",
    "        neighbors = sm_df.loc[word].sort_values(ascending=False).head(N)\n",
    "        # Return list of (neighbor_word, similarity_score) tuples\n",
    "        return list(neighbors.items())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Maximum similarity of neighborhood\n",
    "def max_similarity_of_neighborhoods(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    max_sim1 = max([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    max_sim2 = max([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    return max(max_sim1, max_sim2)\n",
    "\n",
    "# Correlation of similarities of neighborhoods\n",
    "def correlation_of_neighborhoods(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    similarities1 = np.array([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    similarities2 = np.array([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    first_cor = np.corrcoef(similarities1, similarities2)[0, 1]\n",
    "\n",
    "    similarities3 = np.array([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    similarities4 = np.array([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "\n",
    "    second_cor = np.corrcoef(similarities3, similarities4)[0, 1]\n",
    "\n",
    "    return max(first_cor, second_cor)\n",
    "\n",
    "# Sum of squared similarities of neighborhoods\n",
    "def sum_squared_neighborhood_similarities(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    similarities1 = np.array([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    similarities2 = np.array([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    sum_1 = sum(similarities1**2)\n",
    "    sum_2 = sum(similarities2**2)\n",
    "    \n",
    "    return np.sqrt(sum_1 + sum_2)\n",
    "\n",
    "word1 = \"apartment\"\n",
    "word2 = \"great\"\n",
    "N = 3\n",
    "\n",
    "print(f\"For words '{word1}' and '{word2}' and N={N}:\")\n",
    "print(\"Maximum Similarity of Neighborhoods:\", max_similarity_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Correlation of Neighborhoods:\", correlation_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Sum of Squared Neighborhood Similarities:\", sum_squared_neighborhood_similarities(word1, word2, similarity_df, N))\n",
    "\n",
    "\n",
    "word1 = \"apartment\"\n",
    "word2 = \"great\"\n",
    "N = 10\n",
    "\n",
    "print(f\"\\nFor words '{word1}' and '{word2}' and N={N}:\")\n",
    "print(\"Maximum Similarity of Neighborhoods:\", max_similarity_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Correlation of Neighborhoods:\", correlation_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Sum of Squared Neighborhood Similarities:\", sum_squared_neighborhood_similarities(word1, word2, similarity_df, N))\n",
    "\n",
    "\n",
    "print(f\"\\nFor n={N}, neighbors of '{word2}' and it's similarities are:\")\n",
    "print(top_n_neighbors(word2, similarity_df, N))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
