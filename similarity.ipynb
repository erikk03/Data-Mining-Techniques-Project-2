{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ΜΕΛΗ ΟΜΑΔΑΣ: \\\n",
    "ΚΑΓΙΑΤΣΚΑ ΕΡΙΚ - 1115202100043 \\\n",
    "ΚΑΛΑΜΠΟΚΗΣ ΕΥΑΓΓΕΛΟΣ - 1115202100045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows dropped: 92\n",
      "Most common words:\n",
      " ['apartment', 'great', 'us', 'athens', 'stay', 'place', 'location', 'host', 'everything', 'clean', 'nice', 'would', 'recommend', 'really', 'good', 'well', 'perfect', 'helpful', 'also', 'acropolis', 'close', 'walk', 'time', 'comfortable', 'metro', 'restaurants', 'flat', 'amazing', 'area', 'city', 'even', 'easy', 'view', 'walking', 'highly', 'definitely', 'wonderful', 'located', 'one', 'around', 'need', 'lovely', 'get', 'like', 'home', 'airport', 'beautiful', 'best', 'distance', 'back', 'made', 'night', 'spacious', 'could', 'neighborhood', 'quiet', 'br', 'needed', 'excellent', 'staying', 'house', 'friendly', 'experience', 'minutes', 'thank', 'station', 'room', 'gave', 'much', 'day', 'enjoyed', 'stayed', 'away', 'street', 'go', 'fantastic', 'thanks', 'right', 'many', 'places', 'loved', 'within', 'little', 'kind', 'kitchen', 'central', 'visit', 'center', 'two', 'super', 'feel', 'felt', 'recommended', 'balcony', 'airbnb', 'see', 'arrived', 'local', 'lot', 'bed', 'safe', 'taxi', 'provided', 'space', 'arrival', 'small', 'next', 'trip', 'make', 'first', 'bars', 'convenient', 'communication', 'near', 'short', 'didnt', 'come', 'days', 'met', 'better', 'main', 'equipped', 'shops', 'always', 'people', 'lots', 'family', 'sites', 'check', 'sure', 'way', 'extremely', 'every', 'plaka', 'find', 'want', 'brthe', 'enough', 'terrace', 'late', 'nearby', 'appartment', 'minute', 'food', 'help', 'dimitris', 'welcoming', 'big', 'welcome', 'neighbourhood', 'absolutely', 'modern', 'meet', 'exactly', 'bit', 'information', 'questions', 'quite', 'pictures', 'coffee', 'attractions', 'things', 'hosts', 'bus', 'recommendations', 'helped', 'tips', 'bakery', 'views', 'available', 'anyone', 'take', 'breakfast', 'etc', 'centre', 'use', 'bathroom', 'eat', 'though', 'left', 'got', 'went', 'overall', 'min', 'quick', 'took', 'amenities', 'ralou', 'cozy', 'tourist', 'cafes', 'square', 'air', 'building', 'sights', 'living', 'nights', 'greek', 'warm', 'wifi', 'able', 'morning', 'decorated', 'full', 'found', 'dont', 'responsive', 'shower', 'arranged', 'person', 'large', 'described', 'plenty', 'hot', 'ever', 'friends', 'outside', 'aris', 'accommodating', 'greece', 'bedroom', 'far', 'couldnt', 'public', 'showed', 'romanos', 'transportation', 'door', 'easily', 'perfectly', 'awesome', 'museum', 'shopping', 'enjoy', 'cool', 'couple', 'friend', 'restaurant', 'fotis', 'transport', 'looking', 'welcomed', 'part', 'major', 'water', 'town', 'walked', 'return', 'supermarket', 'pleasant', 'love', 'heart', 'problem', 'know', 'anything', 'market', 'photos', 'value', 'corner', 'new', 'haris', 'including', 'syntagma', 'floor', 'across', 'wine', 'although', 'cant', 'worked', 'book', 'everywhere', 'appreciated', 'long', 'cosy', 'hospitality', 'early', 'especially', 'spent', 'say', 'going', 'map', 'never', 'top', 'price', 'anna', 'access', 'visiting', 'stop', 'rooftop', 'fully', 'driver', 'waiting', 'flight', 'incredible', 'maria', 'ilias', 'old', 'thing', 'happy', 'still']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "train_2019_df = pd.read_csv('data_train/train_2019.csv')\n",
    "train_2023_df = pd.read_csv('data_train/train_2023.csv')\n",
    "\n",
    "# Concatenate them in one dataframe\n",
    "df = pd.concat([train_2019_df, train_2023_df], ignore_index=True)\n",
    "\n",
    "# We want to drop lanes that are duplicate\n",
    "initial_row_count = df.shape[0]                                 # Initial row count\n",
    "df = df.drop_duplicates(subset=['id', 'comments'], keep='first')  # Remove duplicates that are shown in both 2019 and 2023\n",
    "final_row_count = df.shape[0]                                   # Final row count\n",
    "rows_dropped = initial_row_count - final_row_count              # Rows dropped\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")                # Print number of rows dropped\n",
    "\n",
    "# Edit comments\n",
    "comments = df['comments'].dropna().astype(str).tolist()\n",
    "\n",
    "# Add all comments in one text\n",
    "all_comments = ' '.join(comments)\n",
    "\n",
    "# Remove special characters etc.\n",
    "words = re.findall(r'\\b\\w+\\b', all_comments.lower())\n",
    "\n",
    "# Find how many times each word exists by counting them\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 300 most used words\n",
    "most_common_words = [word for word, count in word_counts.most_common(300)]\n",
    "\n",
    "# Print them\n",
    "print(\"Most common words:\\n\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Train Word2Vec model with all words\n",
    "model = Word2Vec([words], vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# embeddings for most used words\n",
    "embeddings = {word: model.wv[word] for word in most_common_words if word in model.wv}\n",
    "\n",
    "# similarity matrix\n",
    "word_list = list(embeddings.keys())\n",
    "vectors = np.array([embeddings[word] for word in word_list])\n",
    "similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Create dataframe with similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=word_list, columns=word_list)\n",
    "\n",
    "# Save to pickle file\n",
    "similarity_df.to_pickle('data_similarity/similarity_matrix.pkl')\n",
    "\n",
    "# Save also to csv file\n",
    "similarity_df.to_csv('data_similarity/similarity_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'apartment' embedding:\n",
      " [-0.07819521 -0.04649974 -0.0350303  -0.19499712 -0.09973695 -0.11212839\n",
      "  0.24799238  0.3445658  -0.32557988 -0.1451737  -0.12056579 -0.25358903\n",
      "  0.10670515  0.10988517 -0.08064184  0.12573186 -0.09241755  0.16522697\n",
      " -0.34453133 -0.05753677  0.18802524  0.19304834  0.13482523 -0.22676836\n",
      "  0.08581773  0.14687108 -0.08488967 -0.08562099 -0.3848446  -0.11507275\n",
      " -0.1254433  -0.20515294  0.04733507  0.03186057 -0.07379585  0.27417827\n",
      "  0.07303321  0.07910142  0.27029976 -0.14123401  0.16486734 -0.07165905\n",
      " -0.0663771   0.09932023  0.33153063 -0.05610104 -0.04067687 -0.05535131\n",
      "  0.16023691 -0.03112776]\n",
      "\n",
      "'example' embedding:\n",
      " [-0.37703016  0.07417317 -0.96813391 -0.65633938 -0.42028431  0.5741845\n",
      " -0.54542296 -0.4217084   0.93441761  0.07287706 -0.25512392  0.71332215\n",
      " -0.72980057  0.73043407  0.00705634  0.32479506  0.56099063 -0.93078315\n",
      "  0.99997873  0.88800785 -0.7140831   0.09549437 -0.22583657  0.11740727\n",
      "  0.90161081  0.33430726 -0.76605384 -0.44464761  0.63150544  0.49539474\n",
      "  0.58756619  0.49161237  0.34199144 -0.39582599 -0.01439533  0.71405469\n",
      " -0.43764881 -0.02596505 -0.40312668  0.69874645 -0.59201944 -0.87648321\n",
      " -0.7071856   0.70547733  0.03902589  0.47891944 -0.43995768  0.65186814\n",
      " -0.4916105   0.39159806]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_word_embedding(word, embedding_dict, vector_size=50):\n",
    "    if word in embedding_dict:\n",
    "        return embedding_dict[word]\n",
    "    else:\n",
    "        return np.random.uniform(-1, 1, vector_size)\n",
    "\n",
    "# Example of word that exists\n",
    "word = 'apartment'\n",
    "embedding = get_word_embedding(word, embeddings)\n",
    "print(\"'apartment' embedding:\\n\", embedding)\n",
    "\n",
    "# Example of word that does not exist\n",
    "word = 'example'\n",
    "embedding = get_word_embedding(word, embeddings)\n",
    "print(\"\\n'example' embedding:\\n\", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between apartment and example is:\n",
      "-0.12542367247911174\n",
      "Similarity between apartment and apartment is:\n",
      "0.99999994\n"
     ]
    }
   ],
   "source": [
    "def word_similarity(word1, word2, df_similarity):\n",
    "    # Check if words are in the DataFrame index (assuming both are present)\n",
    "    if word1 in df_similarity.index and word2 in df_similarity.index:\n",
    "        return df_similarity.loc[word1, word2]\n",
    "    else:\n",
    "        emb1 = get_word_embedding(word1, embeddings)\n",
    "        emb2 = get_word_embedding(word2, embeddings)\n",
    "        cos = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "        return cos\n",
    "    \n",
    "word1 = \"apartment\"\n",
    "word2 = \"example\"\n",
    "print(f\"Similarity between {word1} and {word2} is:\")\n",
    "print(word_similarity(word1, word2, similarity_df))     # We expect each time different result because words do not exist\n",
    "\n",
    "word3 = \"apartment\"\n",
    "word4 = \"apartment\"\n",
    "print(f\"Similarity between {word3} and {word4} is:\")    # We expect same result because words exist.\n",
    "print(word_similarity(word3, word4, similarity_df))     # Also approximately 1 because we have the same 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Similarity of Neighborhoods: None\n",
      "Correlation of Neighborhoods: None\n",
      "Sum of Squared Neighborhood Similarities: None\n"
     ]
    }
   ],
   "source": [
    "# Semantic Neighbors\n",
    "def top_n_neighbors(word, sm_df, N):\n",
    "    if word in sm_df.index:\n",
    "        # Get the row corresponding to the word from sm_df\n",
    "        neighbors = sm_df.loc[word].sort_values(ascending=False).head(N)\n",
    "        # Return list of (neighbor_word, similarity_score) tuples\n",
    "        return list(neighbors.items())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Maximum similarity of neighborhood\n",
    "def max_similarity_of_neighborhoods(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    max_sim1 = max([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    max_sim2 = max([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    return max(max_sim1, max_sim2)\n",
    "\n",
    "# Correlation of similarities of neighborhoods\n",
    "def correlation_of_neighborhoods(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    similarities1 = np.array([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    similarities2 = np.array([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    return np.corrcoef(similarities1, similarities2)[0, 1]\n",
    "\n",
    "# Sum of squared similarities of neighborhoods\n",
    "def sum_squared_neighborhood_similarities(word1, word2, sm_df, N):\n",
    "    neighbors1 = top_n_neighbors(word1, sm_df, N)\n",
    "    neighbors2 = top_n_neighbors(word2, sm_df, N)\n",
    "    \n",
    "    if not neighbors1 or not neighbors2:\n",
    "        return None\n",
    "    \n",
    "    similarities1 = np.array([word_similarity(word1, neighbor, sm_df) for neighbor, _ in neighbors2])\n",
    "    similarities2 = np.array([word_similarity(word2, neighbor, sm_df) for neighbor, _ in neighbors1])\n",
    "    \n",
    "    return np.sqrt(np.sum(similarities1**2 + similarities2**2))\n",
    "\n",
    "word1 = \"apartment\"\n",
    "word2 = \"apartment\"\n",
    "N = 5\n",
    "\n",
    "print(\"Maximum Similarity of Neighborhoods:\", max_similarity_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Correlation of Neighborhoods:\", correlation_of_neighborhoods(word1, word2, similarity_df, N))\n",
    "print(\"Sum of Squared Neighborhood Similarities:\", sum_squared_neighborhood_similarities(word1, word2, similarity_df, N))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
