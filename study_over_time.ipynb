{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaggelis_kalabokis/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2019:\n",
      "Data from all months has been written to data_train/train_2019.csv\n",
      "The output CSV file has 2 columns.\n",
      "Number of NaN values replaced: 0\n",
      "\n",
      "For 2023:\n",
      "Data from all months has been written to data_train/train_2023.csv\n",
      "The output CSV file has 2 columns.\n",
      "Number of NaN values replaced: 0\n"
     ]
    }
   ],
   "source": [
    "# Function for output csv file\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "def data_exploration(df_combined, months_folders, output_csv_path, columns_to_select):\n",
    "\n",
    "    # Number of NaN values replaced\n",
    "    nan_replaced_count = 0\n",
    "\n",
    "    # For each month\n",
    "    for month_folder in months_folders:\n",
    "        # Start by importing \"listings.csv\"\n",
    "        input_csv_path = os.path.join(month_folder, 'listings.csv')\n",
    "        \n",
    "        # The dataframe\n",
    "        df = pd.read_csv(input_csv_path, low_memory=False)\n",
    "        \n",
    "        # Include only the columns that we want and are in the .csv file\n",
    "        columns_to_keep = [col for col in columns_to_select if col in df.columns]\n",
    "        df_selected = df[columns_to_keep]\n",
    "        \n",
    "        # Columns from other .csv files\n",
    "        additional_columns = set()\n",
    "        \n",
    "        # Check all other .csv files in the folder\n",
    "        for filename in os.listdir(month_folder):\n",
    "            if filename.endswith('.csv') and filename != 'listings.csv':  # We want .csv files but no the starting one, aka \"listings.csv\"\n",
    "                file_path = os.path.join(month_folder, filename)\n",
    "                other_df = pd.read_csv(file_path)\n",
    "\n",
    "                # Iterate over each column\n",
    "                for col in other_df.columns:\n",
    "                    # If we find a column that we want but is not already in our dataframe, add it from the other .csv file\n",
    "                    if col not in df_selected.columns and col in columns_to_select:\n",
    "                        additional_columns.add(col)\n",
    "                        # Add the column and its data to our dataframe\n",
    "                        df_selected[col] = other_df[col]\n",
    "\n",
    "                    elif  col in df_selected.columns and col in columns_to_select:\n",
    "                        # Check if any value of the column in our dataframe is NaN\n",
    "                        nan_mask = df_selected[col].isna()\n",
    "                        \n",
    "                        # Check if the corresponding value in the other dataframes is not NaN\n",
    "                        non_nan_mask = ~other_df[col].isna()\n",
    "                        \n",
    "                        # Replace NaN values with not NaN ones\n",
    "                        df_selected.loc[nan_mask & non_nan_mask, col] = other_df.loc[nan_mask & non_nan_mask, col]\n",
    "\n",
    "                        # Number of NaN values replaced\n",
    "                        nan_replaced_count += sum(nan_mask & non_nan_mask)\n",
    "\n",
    "        # Concatenate every months dataframes\n",
    "        df_combined = pd.concat([df_combined, df_selected], ignore_index=True)\n",
    "\n",
    "    # Write the final dataframe to the output .csv file\n",
    "    df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Data from all months has been written to {output_csv_path}\")\n",
    "    print(f\"The output CSV file has {df_selected.shape[1]} columns.\")\n",
    "    print(f\"Number of NaN values replaced: {nan_replaced_count}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "# Initialize a DataFrame to store data for 2019\n",
    "dataframe_2019 = pd.DataFrame()\n",
    "\n",
    "# Initialize a DataFrame to store data for 2023\n",
    "dataframe_2023 = pd.DataFrame()\n",
    "\n",
    "# Columns we want to select\n",
    "columns_to_select_2019 = ['id', 'comments']\n",
    "\n",
    "columns_to_select_2023 = ['id', 'comments']\n",
    "\n",
    "### 2019 ###\n",
    "\n",
    "# Each month's folder\n",
    "months_folders = ['data/2019/april', 'data/2019/febrouary', 'data/2019/march']\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path_2019 = 'data_train/train_2019.csv'\n",
    "\n",
    "print(\"For 2019:\")\n",
    "dataframe_2019 = data_exploration(dataframe_2019, months_folders, output_csv_path_2019, columns_to_select_2019)\n",
    "\n",
    "### 2023 ###\n",
    "\n",
    "# Each month's folder\n",
    "months_folders = ['data/2023/june', 'data/2023/march', 'data/2023/september']\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path_2023 = 'data_train/train_2023.csv'\n",
    "\n",
    "print(\"\\nFor 2023:\")\n",
    "dataframe_2023 = data_exploration(dataframe_2023, months_folders, output_csv_path_2023, columns_to_select_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vaggelis_kalabokis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Προεπεξεργασία σχολίων\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση αριθμών\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Αφαίρεση σημείων στίξης και μετατροπή σε μικρούς χαρακτήρες\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Αφαίρεση URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_stopwords(text, stop_words):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_special_tags(text):\n",
    "    # Αφαίρεση HTML tags\n",
    "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Αφαίρεση όλων των χαρακτήρων εκτός από διαστήματα και αγγλικούς χαρακτήρες\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "dataframe_2019.dropna(subset=['comments'], inplace=True)\n",
    "dataframe_2023.dropna(subset=['comments'], inplace=True)\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(preprocess_text)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(preprocess_text)\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(filter_stopwords, stop_words = stop_words)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(filter_stopwords, stop_words = stop_words)\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(clean_special_tags)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(clean_special_tags)\n",
    "\n",
    "dataframe_2019 = dataframe_2019[dataframe_2019['comments'].str.strip().astype(bool)]\n",
    "dataframe_2023 = dataframe_2023[dataframe_2023['comments'].str.strip().astype(bool)]\n",
    "\n",
    "dataframe_2019.to_csv(output_csv_path_2019, index=False)\n",
    "dataframe_2023.to_csv(output_csv_path_2023, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts for 2019:\n",
      "sentiment\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment counts for 2023:\n",
      "sentiment\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Φόρτωση μοντέλου ανάλυσης συναισθήματος\n",
    "# sentiment_analysis = pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Load the sentiment analysis model and tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_analysis = pipeline('sentiment-analysis', model=model_name, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    results = []\n",
    "    for idx, comment in zip(data['id'], data['comments']):\n",
    "        result = sentiment_analysis(comment)[0]\n",
    "        sentiment = result['label']\n",
    "        if sentiment == 'LABEL_0':\n",
    "            sentiment = 'negative'\n",
    "        elif sentiment == 'LABEL_1':\n",
    "            sentiment = 'neutral'\n",
    "        else:\n",
    "            sentiment = 'positive'\n",
    "        results.append({'id': idx, 'review': comment, 'sentiment': sentiment})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ανάλυση συναισθήματος για τα έτη 2019 και 2023\n",
    "sample_df_2019 = dataframe_2019.sample(n=1000)\n",
    "sample_df_2023 = dataframe_2023.sample(n=1000)\n",
    "\n",
    "sentiment_df_2019 = analyze_sentiment(sample_df_2019[['id', 'comments']])\n",
    "sentiment_df_2023 = analyze_sentiment(sample_df_2023[['id', 'comments']])\n",
    "\n",
    "sentiment_df_2019.to_csv(\"data_sentiment/sentiment_2019.csv\", index = False)\n",
    "sentiment_df_2023.to_csv(\"data_sentiment/sentiment_2023.csv\", index = False)\n",
    "\n",
    "# Calculate sentiment counts\n",
    "sentiment_counts_2019 = sentiment_df_2019['sentiment'].value_counts()\n",
    "sentiment_counts_2023 = sentiment_df_2023['sentiment'].value_counts()\n",
    "\n",
    "# Print sentiment counts\n",
    "print(\"Sentiment counts for 2019:\")\n",
    "print(sentiment_counts_2019)\n",
    "print(\"\\nSentiment counts for 2023:\")\n",
    "print(sentiment_counts_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts for 2019:\n",
      "sentiment\n",
      "positive    2596\n",
      "neutral      507\n",
      "negative      42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment counts for 2023:\n",
      "sentiment\n",
      "positive    2648\n",
      "neutral      466\n",
      "negative      18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# positive_words = [\"great\", \"awesome\", \"glad\", \"happy\", \"fantastic\", \"good\", \"excellent\", \"amazing\"]\n",
    "# negative_words = [\"terrible\", \"awful\", \"gross\", \"dirty\", \"disappointed\", \"bad\", \"poor\", \"horrible\"]\n",
    "\n",
    "\n",
    "\n",
    "positive_words = [\"great\", \"awesome\", \"glad\", \"happy\", \"fantastic\", \"good\", \"excellent\", \"amazing\",\n",
    "                  \"beautiful\", \"wonderful\", \"superb\", \"perfect\", \"delightful\", \"joyful\", \"splendid\",\n",
    "                  \"marvelous\", \"terrific\", \"brilliant\", \"phenomenal\", \"fabulous\", \"vibrant\", \"positive\",\n",
    "                  \"uplifting\", \"lovely\", \"ideal\", \"satisfying\", \"outstanding\", \"magnificent\", \"stellar\",\n",
    "                  \"glorious\", \"radiant\", \"blissful\", \"ecstatic\", \"content\", \"thrilled\", \"charming\", \"sweet\",\n",
    "                  \"heartwarming\", \"kind\", \"grateful\", \"optimistic\", \"inspiring\", \"remarkable\", \"captivating\"]\n",
    "\n",
    "negative_words = [\"terrible\", \"awful\", \"gross\", \"dirty\", \"disappointed\", \"bad\", \"poor\", \"horrible\",\n",
    "                  \"unpleasant\", \"unfortunate\", \"miserable\", \"distressing\", \"inferior\", \"grim\", \"bleak\",\n",
    "                  \"unacceptable\", \"dreadful\", \"lousy\", \"painful\", \"sorrowful\", \"regrettable\", \"tragic\",\n",
    "                  \"depressing\", \"dismal\", \"unsatisfactory\", \"heartbreaking\", \"unfavorable\", \"atrocious\",\n",
    "                  \"abysmal\", \"deplorable\", \"pitiful\", \"abominable\", \"gloomy\", \"disheartening\", \"dreary\",\n",
    "                  \"negative\", \"displeasing\", \"repugnant\", \"appalling\", \"detestable\", \"horrifying\", \"dire\",\n",
    "                  \"shameful\", \"wretched\", \"unsuitable\", \"disgusting\", \"offensive\"]\n",
    "\n",
    "\n",
    "def contains_positive_words(text):\n",
    "    return any(word in text for word in positive_words)\n",
    "\n",
    "def contains_negative_words(text):\n",
    "    return any(word in text for word in negative_words)\n",
    "\n",
    "# Load model, tokenizer, and config\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(data):\n",
    "    results = []\n",
    "    for idx, comment in zip(data['id'], data['comments']):\n",
    "        comment = preprocess(comment)\n",
    "        encoded_input = tokenizer(comment, return_tensors='pt', truncation=True, max_length=128)\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        ranking = np.argsort(scores)[::-1]\n",
    "        sentiment = config.id2label[ranking[0]]\n",
    "        results.append({'id': idx, 'review': comment, 'sentiment': sentiment})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Sample data for analysis\n",
    "# Assuming dataframe_2019 and dataframe_2023 are already defined\n",
    "sample_df_2019 = dataframe_2019.sample(n=3000)\n",
    "sample_df_2023 = dataframe_2023.sample(n=3000)\n",
    "\n",
    "\n",
    "\n",
    "positive_reviews_2019 = sample_df_2019[sample_df_2019['comments'].apply(contains_positive_words)]\n",
    "negative_reviews_2019 = sample_df_2019[sample_df_2019['comments'].apply(contains_negative_words)]\n",
    "neutral_reviews_2019 = sample_df_2019[~(sample_df_2019['comments'].apply(contains_positive_words) | sample_df_2019['comments'].apply(contains_negative_words))]\n",
    "\n",
    "\n",
    "positive_reviews_2023 = sample_df_2023[sample_df_2023['comments'].apply(contains_positive_words)]\n",
    "negative_reviews_2023 = sample_df_2023[sample_df_2023['comments'].apply(contains_negative_words)]\n",
    "neutral_reviews_2023 = sample_df_2023[~(sample_df_2023['comments'].apply(contains_positive_words) | sample_df_2023['comments'].apply(contains_negative_words))]\n",
    "\n",
    "sentiment_df_positive_2019 = analyze_sentiment(positive_reviews_2019[['id', 'comments']])\n",
    "sentiment_df_negative_2019 = analyze_sentiment(negative_reviews_2019[['id', 'comments']])\n",
    "sentiment_df_neutral_2019 = analyze_sentiment(neutral_reviews_2019[['id', 'comments']])\n",
    "\n",
    "sentiment_df_positive_2023 = analyze_sentiment(positive_reviews_2023[['id', 'comments']])\n",
    "sentiment_df_negative_2023 = analyze_sentiment(negative_reviews_2023[['id', 'comments']])\n",
    "sentiment_df_neutral_2023 = analyze_sentiment(neutral_reviews_2023[['id', 'comments']])\n",
    "\n",
    "# # Analyze sentiment for the sampled data\n",
    "# sentiment_df_2019 = analyze_sentiment(sample_df_2019[['id', 'comments']])\n",
    "# sentiment_df_2023 = analyze_sentiment(sample_df_2023[['id', 'comments']])\n",
    "sentiment_df_2019 = pd.concat([sentiment_df_positive_2019, sentiment_df_negative_2019, sentiment_df_neutral_2019])\n",
    "sentiment_df_2023 = pd.concat([sentiment_df_positive_2023, sentiment_df_negative_2023, sentiment_df_neutral_2023])\n",
    "\n",
    "sentiment_df_2019.to_csv(\"data_sentiment/sentiment_2019.csv\", index = False)\n",
    "sentiment_df_2023.to_csv(\"data_sentiment/sentiment_2023.csv\", index = False)\n",
    "\n",
    "# Calculate sentiment counts\n",
    "sentiment_counts_2019 = sentiment_df_2019['sentiment'].value_counts()\n",
    "sentiment_counts_2023 = sentiment_df_2023['sentiment'].value_counts()\n",
    "\n",
    "# Print sentiment counts\n",
    "print(\"Sentiment counts for 2019:\")\n",
    "print(sentiment_counts_2019)\n",
    "print(\"\\nSentiment counts for 2023:\")\n",
    "print(sentiment_counts_2023)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
