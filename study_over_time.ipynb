{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for output csv file\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "\n",
    "def data_exploration(df_combined, months_folders, output_csv_path, columns_to_select):\n",
    "\n",
    "    # Number of NaN values replaced\n",
    "    nan_replaced_count = 0\n",
    "\n",
    "    # For each month\n",
    "    for month_folder in months_folders:\n",
    "        # Start by importing \"listings.csv\"\n",
    "        input_csv_path = os.path.join(month_folder, 'listings.csv')\n",
    "        \n",
    "        # The dataframe\n",
    "        df = pd.read_csv(input_csv_path, low_memory=False)\n",
    "        \n",
    "        # Include only the columns that we want and are in the .csv file\n",
    "        columns_to_keep = [col for col in columns_to_select if col in df.columns]\n",
    "        df_selected = df[columns_to_keep]\n",
    "        \n",
    "        # Columns from other .csv files\n",
    "        additional_columns = set()\n",
    "        \n",
    "        # Check all other .csv files in the folder\n",
    "        for filename in os.listdir(month_folder):\n",
    "            if filename.endswith('.csv') and filename != 'listings.csv':  # We want .csv files but no the starting one, aka \"listings.csv\"\n",
    "                file_path = os.path.join(month_folder, filename)\n",
    "                other_df = pd.read_csv(file_path)\n",
    "\n",
    "                # Iterate over each column\n",
    "                for col in other_df.columns:\n",
    "                    # If we find a column that we want but is not already in our dataframe, add it from the other .csv file\n",
    "                    if col not in df_selected.columns and col in columns_to_select:\n",
    "                        additional_columns.add(col)\n",
    "                        # Add the column and its data to our dataframe\n",
    "                        df_selected[col] = other_df[col]\n",
    "\n",
    "                    elif  col in df_selected.columns and col in columns_to_select:\n",
    "                        # Check if any value of the column in our dataframe is NaN\n",
    "                        nan_mask = df_selected[col].isna()\n",
    "                        \n",
    "                        # Check if the corresponding value in the other dataframes is not NaN\n",
    "                        non_nan_mask = ~other_df[col].isna()\n",
    "                        \n",
    "                        # Replace NaN values with not NaN ones\n",
    "                        df_selected.loc[nan_mask & non_nan_mask, col] = other_df.loc[nan_mask & non_nan_mask, col]\n",
    "\n",
    "                        # Number of NaN values replaced\n",
    "                        nan_replaced_count += sum(nan_mask & non_nan_mask)\n",
    "\n",
    "        # Concatenate every months dataframes\n",
    "        df_combined = pd.concat([df_combined, df_selected], ignore_index=True)\n",
    "\n",
    "    # Write the final dataframe to the output .csv file\n",
    "    df_combined.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Data from all months has been written to {output_csv_path}\")\n",
    "    print(f\"The output CSV file has {df_selected.shape[1]} columns.\")\n",
    "    print(f\"Number of NaN values replaced: {nan_replaced_count}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "# Initialize a DataFrame to store data for 2019\n",
    "dataframe_2019 = pd.DataFrame()\n",
    "\n",
    "# Initialize a DataFrame to store data for 2023\n",
    "dataframe_2023 = pd.DataFrame()\n",
    "\n",
    "# Columns we want to select\n",
    "columns_to_select_2019 = ['id', 'comments']\n",
    "\n",
    "columns_to_select_2023 = ['id', 'comments']\n",
    "\n",
    "### 2019 ###\n",
    "\n",
    "# Each month's folder\n",
    "months_folders = ['data/2019/april', 'data/2019/febrouary', 'data/2019/march']\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path_2019 = 'data_train/train_2019.csv'\n",
    "\n",
    "print(\"For 2019:\")\n",
    "dataframe_2019 = data_exploration(dataframe_2019, months_folders, output_csv_path_2019, columns_to_select_2019)\n",
    "\n",
    "### 2023 ###\n",
    "\n",
    "# Each month's folder\n",
    "months_folders = ['data/2023/june', 'data/2023/march', 'data/2023/september']\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv_path_2023 = 'data_train/train_2023.csv'\n",
    "\n",
    "print(\"\\nFor 2023:\")\n",
    "dataframe_2023 = data_exploration(dataframe_2023, months_folders, output_csv_path_2023, columns_to_select_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Προεπεξεργασία σχολίων\n",
    "def preprocess_text(text):\n",
    "    # Αφαίρεση αριθμών\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Αφαίρεση σημείων στίξης και μετατροπή σε μικρούς χαρακτήρες\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Αφαίρεση URL\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_stopwords(text, stop_words):\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_special_tags(text):\n",
    "    # Αφαίρεση HTML tags\n",
    "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Αφαίρεση όλων των χαρακτήρων εκτός από διαστήματα και αγγλικούς χαρακτήρες\n",
    "    clean_text = re.sub(r'[^a-zA-Z\\s]', '', clean_text)\n",
    "    return clean_text\n",
    "\n",
    "dataframe_2019.dropna(subset=['comments'], inplace=True)\n",
    "dataframe_2023.dropna(subset=['comments'], inplace=True)\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(preprocess_text)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(preprocess_text)\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(filter_stopwords, stop_words = stop_words)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(filter_stopwords, stop_words = stop_words)\n",
    "\n",
    "dataframe_2019['comments'] = dataframe_2019['comments'].apply(clean_special_tags)\n",
    "dataframe_2023['comments'] = dataframe_2023['comments'].apply(clean_special_tags)\n",
    "\n",
    "dataframe_2019 = dataframe_2019[dataframe_2019['comments'].str.strip().astype(bool)]\n",
    "dataframe_2023 = dataframe_2023[dataframe_2023['comments'].str.strip().astype(bool)]\n",
    "\n",
    "dataframe_2019.to_csv(output_csv_path_2019, index=False)\n",
    "dataframe_2023.to_csv(output_csv_path_2023, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment counts for 2019:\n",
      "sentiment\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment counts for 2023:\n",
      "sentiment\n",
      "positive    1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Φόρτωση μοντέλου ανάλυσης συναισθήματος\n",
    "# sentiment_analysis = pipeline('sentiment-analysis', model='siebert/sentiment-roberta-large-english')\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Load the sentiment analysis model and tokenizer\n",
    "model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_analysis = pipeline('sentiment-analysis', model=model_name, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    results = []\n",
    "    for idx, comment in zip(data['id'], data['comments']):\n",
    "        result = sentiment_analysis(comment)[0]\n",
    "        sentiment = result['label']\n",
    "        if sentiment == 'LABEL_0':\n",
    "            sentiment = 'negative'\n",
    "        elif sentiment == 'LABEL_1':\n",
    "            sentiment = 'neutral'\n",
    "        else:\n",
    "            sentiment = 'positive'\n",
    "        results.append({'id': idx, 'review': comment, 'sentiment': sentiment})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ανάλυση συναισθήματος για τα έτη 2019 και 2023\n",
    "sample_df_2019 = dataframe_2019.sample(n=1000)\n",
    "sample_df_2023 = dataframe_2023.sample(n=1000)\n",
    "\n",
    "sentiment_df_2019 = analyze_sentiment(sample_df_2019[['id', 'comments']])\n",
    "sentiment_df_2023 = analyze_sentiment(sample_df_2023[['id', 'comments']])\n",
    "\n",
    "sentiment_df_2019.to_csv(\"data_sentiment/sentiment_2019.csv\", index = False)\n",
    "sentiment_df_2023.to_csv(\"data_sentiment/sentiment_2023.csv\", index = False)\n",
    "\n",
    "# Calculate sentiment counts\n",
    "sentiment_counts_2019 = sentiment_df_2019['sentiment'].value_counts()\n",
    "sentiment_counts_2023 = sentiment_df_2023['sentiment'].value_counts()\n",
    "\n",
    "# Print sentiment counts\n",
    "print(\"Sentiment counts for 2019:\")\n",
    "print(sentiment_counts_2019)\n",
    "print(\"\\nSentiment counts for 2023:\")\n",
    "print(sentiment_counts_2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
