{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ΜΕΛΗ ΟΜΑΔΑΣ: \\\n",
    "ΚΑΓΙΑΤΣΚΑ ΕΡΙΚ - 1115202100043 \\\n",
    "ΚΑΛΑΜΠΟΚΗΣ ΕΥΑΓΓΕΛΟΣ - 1115202100045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import from CSV sentiment data\n",
    "sentiment_df_2019 = pd.read_csv(\"data_sentiment/sentiment_2019.csv\", low_memory=False)\n",
    "sentiment_df_2023 = pd.read_csv(\"data_sentiment/sentiment_2023.csv\", low_memory=False)\n",
    "\n",
    "# Concatenate sentiment_df_2019 and sentiment_df_2023 in one dataframe\n",
    "df = pd.concat([sentiment_df_2019, sentiment_df_2023])\n",
    "\n",
    "# We want to drop lanes that are duplicate\n",
    "initial_row_count = df.shape[0]                                 # Initial row count\n",
    "df = df.drop_duplicates(subset=['id', 'review'], keep='first')  # Remove duplicates that are shown in both 2019 and 2023\n",
    "final_row_count = df.shape[0]                                   # Final row count\n",
    "rows_dropped = initial_row_count - final_row_count              # Rows dropped\n",
    "print(f\"Number of rows dropped: {rows_dropped}\")                # Print number of rows dropped\n",
    "\n",
    "# Split data: training (80%) and testing (20%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Drop some data so we have good analogy between positive, negative and neutral sentiments\n",
    "positive_comments_train = train_df[train_df['sentiment'] == 'positive'].sample(n=400, random_state=42)\n",
    "train_df = train_df.drop(positive_comments_train.index)\n",
    "\n",
    "positive_comments_test = test_df[test_df['sentiment'] == 'positive'].sample(n=0, random_state=42)\n",
    "test_df = test_df.drop(positive_comments_test.index)\n",
    "\n",
    "# Print sentiments counts\n",
    "print(\"Train value counts:\")\n",
    "display(train_df['sentiment'].value_counts())\n",
    "print(\"Test value counts:\")\n",
    "display(test_df['sentiment'].value_counts())\n",
    "\n",
    "\n",
    "# Save to the tsv files\n",
    "train_df.to_csv('data_tsv/train.tsv', index=False)\n",
    "test_df.to_csv('data_tsv/test.tsv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tf-idf Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_df['review'])\n",
    "tfidf_test = tfidf_vectorizer.transform(test_df['review'])\n",
    "\n",
    "# Save/store Tf-idf characteristics\n",
    "with open('data_pkl/tfidf_train.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_train, f)\n",
    "with open('data_pkl/tfidf_test.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_test, f)\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "tokenized_reviews = [review.split() for review in train_df['review']]\n",
    "word2vec_model = Word2Vec(tokenized_reviews, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extract embeddings from Word2Vec model\n",
    "def get_word2vec_embeddings(reviews, model):\n",
    "    embeddings = []                                                         # List to store the embeddings\n",
    "    for review in reviews:                                                  # Loop through each review in the reviews list\n",
    "        words = review.split()                                              # Split the review into words\n",
    "        word_vecs = [model.wv[word] for word in words if word in model.wv]  # Get each word's vector if it's in vocabulary of the model\n",
    "        if word_vecs:\n",
    "            embeddings.append(np.mean(word_vecs, axis=0))                   # Compute the mean of the word vectors\n",
    "        else:                                                               # If there are no word vectors\n",
    "            embeddings.append(np.zeros(model.vector_size))                  # Append a zero vector\n",
    "    return np.array(embeddings)                                             # Return list of embeddings\n",
    "\n",
    "\n",
    "word2vec_train = get_word2vec_embeddings(train_df['review'], word2vec_model)\n",
    "word2vec_test = get_word2vec_embeddings(test_df['review'], word2vec_model)\n",
    "\n",
    "# Save/store Word2Vec characteristics\n",
    "with open('data_pkl/word2vec_train.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec_train, f)\n",
    "with open('data_pkl/word2vec_test.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "target_names = ['positive', 'negative', 'neutral']\n",
    "\n",
    "# Load characteristics - Tf-idf\n",
    "with open('data_pkl/tfidf_train.pkl', 'rb') as f:\n",
    "    x_tfidf_train = pickle.load(f)\n",
    "with open('data_pkl/tfidf_test.pkl', 'rb') as f:\n",
    "    x_tfidf_test = pickle.load(f)\n",
    "\n",
    "# Load characteristics - Word Embeddings\n",
    "with open('data_pkl/word2vec_train.pkl', 'rb') as f:\n",
    "    x_word2vec_train = pickle.load(f)\n",
    "with open('data_pkl/word2vec_test.pkl', 'rb') as f:\n",
    "    x_word2vec_test = pickle.load(f)\n",
    "\n",
    "# Load labels\n",
    "train_df = pd.read_csv('data_tsv/train.tsv')\n",
    "test_df = pd.read_csv('data_tsv/test.tsv')\n",
    "\n",
    "y_train = train_df['sentiment'] \n",
    "y_test = test_df['sentiment']  \n",
    "\n",
    "# Standardize the Word2Vec features\n",
    "scaler = StandardScaler()\n",
    "x_word2vec_train_scaled = scaler.fit_transform(x_word2vec_train)\n",
    "x_word2vec_test_scaled = scaler.transform(x_word2vec_test)\n",
    "\n",
    "# List od classifiers\n",
    "classifiers = {\n",
    "    \"SVM\": SVC(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "     \"KNN\": KNeighborsClassifier(n_neighbors=15)\n",
    "}\n",
    "\n",
    "def evaluate_classifier(clf, x_train, y_train, x_test, y_test):\n",
    "    # 10-fold Cross Validation\n",
    "    scores = cross_val_score(clf, x_train, y_train, cv=10)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, zero_division=0)\n",
    "\n",
    "    # Final score\n",
    "    test_score = clf.score(x_test, y_test)\n",
    "\n",
    "    # Return all values wanted and classification report\n",
    "    return {\n",
    "        'Results Cross Validation': scores,\n",
    "        'Mean': np.mean(scores),\n",
    "        'Final Results': test_score,\n",
    "        'Classification Report\\n': report\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    # Evaluate with tfidf\n",
    "    print(f\"Evaluating {name} with TFIDF features...\")\n",
    "    tfidf_results = evaluate_classifier(clf, x_tfidf_train, y_train, x_tfidf_test, y_test)\n",
    "    if tfidf_results:\n",
    "        results[f'{name} with TFIDF'] = tfidf_results\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # Evaluate with w2v\n",
    "    print(f\"Evaluating {name} with Word2Vec features...\")\n",
    "    word2vec_results = evaluate_classifier(clf, x_word2vec_train_scaled, y_train, x_word2vec_test_scaled, y_test)\n",
    "    if word2vec_results:\n",
    "        results[f'{name} with standardized Word2Vec'] = word2vec_results\n",
    "    print(\"Done.\")\n",
    "\n",
    "# Print results\n",
    "for key, value in results.items():\n",
    "    print(f\"\\nResults for {key}:\")\n",
    "    for metric, score in value.items():\n",
    "        print(f\"{metric}: {score}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
